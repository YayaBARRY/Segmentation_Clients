{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db6cbe94-db28-4f3a-8a32-fe213a9f39f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Création de la session PySpark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Segmentation_Clients_Expresso\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61cfd990-cd15-48cc-8527-66096f90bd13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+-------------+-------+--------------+-------+------------+---------+-----------+------+------+----+-----+-----+---+----------+--------------------+-------------+-----+\n",
      "|             user_id|REGION|       TENURE|MONTANT|FREQUENCE_RECH|REVENUE|ARPU_SEGMENT|FREQUENCE|DATA_VOLUME|ON_NET|ORANGE|TIGO|ZONE1|ZONE2|MRG|REGULARITY|            TOP_PACK|FREQ_TOP_PACK|CHURN|\n",
      "+--------------------+------+-------------+-------+--------------+-------+------------+---------+-----------+------+------+----+-----+-----+---+----------+--------------------+-------------+-----+\n",
      "|00000bfd7d50f0109...|FATICK| K > 24 month| 4250.0|          15.0| 4251.0|      1417.0|     17.0|        4.0| 388.0|  46.0| 1.0|  1.0|  2.0| NO|        54|On net 200F=Unlim...|          8.0|    0|\n",
      "|00000cb4a5d760de8...|  NULL|I 18-21 month|   NULL|          NULL|   NULL|        NULL|     NULL|       NULL|  NULL|  NULL|NULL| NULL| NULL| NO|         4|                NULL|         NULL|    1|\n",
      "|00001654a9d9f9630...|  NULL| K > 24 month| 3600.0|           2.0| 1020.0|       340.0|      2.0|       NULL|  90.0|  46.0| 7.0| NULL| NULL| NO|        17|On-net 1000F=10Mi...|          1.0|    0|\n",
      "|00001dd6fa45f7ba0...| DAKAR| K > 24 month|13500.0|          15.0|13502.0|      4501.0|     18.0|    43804.0|  41.0| 102.0| 2.0| NULL| NULL| NO|        62|   Data:1000F=5GB,7d|         11.0|    0|\n",
      "|000028d9e13a595ab...| DAKAR| K > 24 month| 1000.0|           1.0|  985.0|       328.0|      1.0|       NULL|  39.0|  24.0|NULL| NULL| NULL| NO|        11|Mixt 250F=Unlimit...|          2.0|    0|\n",
      "+--------------------+------+-------------+-------+--------------+-------+------------+---------+-----------+------+------+----+-----+-----+---+----------+--------------------+-------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Chargement du fichier\n",
    "df = spark.read.csv(\"Expresso_data.csv\", header=True, inferSchema=True)\n",
    "\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5552096a-c633-438a-aa93-1e2383411a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- REGION: string (nullable = true)\n",
      " |-- TENURE: string (nullable = true)\n",
      " |-- MONTANT: double (nullable = true)\n",
      " |-- FREQUENCE_RECH: double (nullable = true)\n",
      " |-- REVENUE: double (nullable = true)\n",
      " |-- ARPU_SEGMENT: double (nullable = true)\n",
      " |-- FREQUENCE: double (nullable = true)\n",
      " |-- DATA_VOLUME: double (nullable = true)\n",
      " |-- ON_NET: double (nullable = true)\n",
      " |-- ORANGE: double (nullable = true)\n",
      " |-- TIGO: double (nullable = true)\n",
      " |-- ZONE1: double (nullable = true)\n",
      " |-- ZONE2: double (nullable = true)\n",
      " |-- MRG: string (nullable = true)\n",
      " |-- REGULARITY: integer (nullable = true)\n",
      " |-- TOP_PACK: string (nullable = true)\n",
      " |-- FREQ_TOP_PACK: double (nullable = true)\n",
      " |-- CHURN: integer (nullable = true)\n",
      "\n",
      "Nombre de lignes : 2154048\n",
      "Nombre de colonnes : 19\n"
     ]
    }
   ],
   "source": [
    "# Affiche les types de colonnes et s'il y a des nulls\n",
    "df.printSchema()\n",
    "\n",
    "print(f\"Nombre de lignes : {df.count()}\")\n",
    "print(f\"Nombre de colonnes : {len(df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "adbc870b-a403-4830-ba55-cd1a9d47ecc2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valeurs Uniques</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>2072614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <td>40472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REVENUE</th>\n",
       "      <td>35650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <td>14786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_NET</th>\n",
       "      <td>8989</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTANT</th>\n",
       "      <td>5910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORANGE</th>\n",
       "      <td>2894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIGO</th>\n",
       "      <td>1288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZONE1</th>\n",
       "      <td>625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZONE2</th>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQ_TOP_PACK</th>\n",
       "      <td>231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOP_PACK</th>\n",
       "      <td>143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQUENCE</th>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGULARITY</th>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION</th>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENURE</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHURN</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRG</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                Valeurs Uniques\n",
       "user_id                 2072614\n",
       "DATA_VOLUME               40472\n",
       "REVENUE                   35650\n",
       "ARPU_SEGMENT              14786\n",
       "ON_NET                     8989\n",
       "MONTANT                    5910\n",
       "ORANGE                     2894\n",
       "TIGO                       1288\n",
       "ZONE1                       625\n",
       "ZONE2                       481\n",
       "FREQ_TOP_PACK               231\n",
       "TOP_PACK                    143\n",
       "FREQUENCE_RECH              123\n",
       "FREQUENCE                    86\n",
       "REGULARITY                   62\n",
       "REGION                       13\n",
       "TENURE                        8\n",
       "CHURN                         2\n",
       "MRG                           1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import approx_count_distinct\n",
    "\n",
    "# Récupération du nombre de valeurs uniques par colonne\n",
    "unique_counts = {\n",
    "    c: df.select(approx_count_distinct(c)).first()[0]\n",
    "    for c in df.columns\n",
    "}\n",
    "\n",
    "# Affichage trié avec Pandas\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(unique_counts, orient=\"index\", columns=[\"Valeurs Uniques\"])\\\n",
    "    .sort_values(\"Valeurs Uniques\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0993996f-6c53-49a0-9ee0-f451efa718f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doublons détectés : 0\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "# Nombre de lignes totales vs distinctes\n",
    "total_rows = df.count()\n",
    "distinct_rows = df.distinct().count()\n",
    "\n",
    "print(f\"Doublons détectés : {total_rows - distinct_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3aa5eeaf-0655-4f18-b417-8e242ba966d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>% Manquant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ZONE2</th>\n",
       "      <td>93.65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ZONE1</th>\n",
       "      <td>92.12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TIGO</th>\n",
       "      <td>59.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATA_VOLUME</th>\n",
       "      <td>49.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQ_TOP_PACK</th>\n",
       "      <td>41.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TOP_PACK</th>\n",
       "      <td>41.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORANGE</th>\n",
       "      <td>41.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION</th>\n",
       "      <td>39.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ON_NET</th>\n",
       "      <td>36.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MONTANT</th>\n",
       "      <td>35.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQUENCE_RECH</th>\n",
       "      <td>35.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REVENUE</th>\n",
       "      <td>33.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ARPU_SEGMENT</th>\n",
       "      <td>33.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREQUENCE</th>\n",
       "      <td>33.71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGULARITY</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>user_id</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MRG</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TENURE</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>CHURN</th>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                % Manquant\n",
       "ZONE2                93.65\n",
       "ZONE1                92.12\n",
       "TIGO                 59.89\n",
       "DATA_VOLUME          49.23\n",
       "FREQ_TOP_PACK        41.90\n",
       "TOP_PACK             41.90\n",
       "ORANGE               41.56\n",
       "REGION               39.43\n",
       "ON_NET               36.52\n",
       "MONTANT              35.13\n",
       "FREQUENCE_RECH       35.13\n",
       "REVENUE              33.71\n",
       "ARPU_SEGMENT         33.71\n",
       "FREQUENCE            33.71\n",
       "REGULARITY            0.00\n",
       "user_id               0.00\n",
       "MRG                   0.00\n",
       "TENURE                0.00\n",
       "CHURN                 0.00"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum, isnan\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# Nombre total de lignes\n",
    "total_rows = df.count()\n",
    "\n",
    "# Initialisation du dictionnaire des pourcentages de valeurs manquantes\n",
    "percent_missing = {}\n",
    "\n",
    "# Traitement colonne par colonne\n",
    "for c in df.columns:\n",
    "    if isinstance(df.schema[c].dataType, (DoubleType, FloatType)):\n",
    "        null_count = df.filter(col(c).isNull() | isnan(col(c))).count()\n",
    "    else:\n",
    "        null_count = df.filter(col(c).isNull()).count()\n",
    "    \n",
    "    pourcentage = (null_count / total_rows) * 100\n",
    "    percent_missing[c] = round(pourcentage, 2)\n",
    "\n",
    "# Affichage trié\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(percent_missing, orient=\"index\", columns=[\"% Manquant\"]).sort_values(\"% Manquant\", ascending=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9d504826-af1a-46dc-8848-2daff91aa54b",
   "metadata": {},
   "source": [
    "# Suppression des lignes avec au moins une valeur manquante\n",
    "df_cleaned = df.dropna(how=\"any\")\n",
    "\n",
    "# Affichage de la forme (nombre de lignes, nombre de colonnes)\n",
    "print(f\"Nombre de lignes après suppression : {df_cleaned.count()}\")\n",
    "print(f\"Nombre de colonnes : {len(df_cleaned.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ed52fb0-78c0-4c56-b994-d6da24ed5e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppression des colonnes choisies\n",
    "df = df.drop(\"user_id\", \"TIGO\", \"ZONE1\", \"ZONE2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95c51134-6c1f-402f-b879-92c25bd095c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-----------+\n",
      "|CHURN|  count|Pourcentage|\n",
      "+-----+-------+-----------+\n",
      "|    1| 403986|      18.75|\n",
      "|    0|1750062|      81.25|\n",
      "+-----+-------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, round\n",
    "\n",
    "# Nombre total de lignes\n",
    "total_rows = df.count()\n",
    "\n",
    "# Calcul du pourcentage par valeur de CHURN\n",
    "df.groupBy(\"CHURN\")\\\n",
    "  .count()\\\n",
    "  .withColumn(\"Pourcentage\", round((col(\"count\") / total_rows) * 100, 2))\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9061481d-f9e9-4141-ad4f-c910f154ba8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variables catégorielles : ['REGION', 'TENURE', 'MRG', 'TOP_PACK']\n",
      "Variables numériques : ['MONTANT', 'FREQUENCE_RECH', 'REVENUE', 'ARPU_SEGMENT', 'FREQUENCE', 'DATA_VOLUME', 'ON_NET', 'ORANGE', 'REGULARITY', 'FREQ_TOP_PACK', 'CHURN']\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StringType, NumericType\n",
    "\n",
    "# Sélection des colonnes de type string (équivalent object/category)\n",
    "cat_vars = [f.name for f in df.schema.fields if isinstance(f.dataType, StringType)]\n",
    "\n",
    "# Sélection des colonnes numériques\n",
    "num_vars = [f.name for f in df.schema.fields if isinstance(f.dataType, NumericType)]\n",
    "\n",
    "print(\"Variables catégorielles :\", cat_vars)\n",
    "print(\"Variables numériques :\", num_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0f4bc0ab-80d0-4b81-acba-e7ff8466f2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, mean, count\n",
    "\n",
    "# 1. Remplacer les valeurs manquantes dans les colonnes numériques par la moyenne\n",
    "for c in num_vars:\n",
    "    mean_value = df.select(mean(col(c))).first()[0]\n",
    "    if mean_value is not None:\n",
    "        df = df.fillna({c: mean_value})\n",
    "\n",
    "# 2. Remplacer les valeurs manquantes dans les colonnes catégorielles par la valeur la plus fréquente (mode)\n",
    "for c in cat_vars:\n",
    "    try:\n",
    "        mode_value = df.groupBy(c).count().orderBy(col(\"count\").desc()).first()[0]\n",
    "        if mode_value is not None:\n",
    "            df = df.fillna({c: mode_value})\n",
    "    except:\n",
    "        print(f\"Impossible de calculer le mode pour la colonne {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35989d7a-d6e1-496c-b83b-2199a14f1899",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Valeurs manquantes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TOP_PACK</th>\n",
       "      <td>902594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>REGION</th>\n",
       "      <td>849299</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Valeurs manquantes\n",
       "TOP_PACK              902594\n",
       "REGION                849299"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, isnan, sum\n",
    "from pyspark.sql.types import DoubleType, FloatType\n",
    "\n",
    "# Calcul du nombre de valeurs manquantes par colonne\n",
    "missing_counts = {}\n",
    "\n",
    "for c in df.columns:\n",
    "    if isinstance(df.schema[c].dataType, (DoubleType, FloatType)):\n",
    "        count_missing = df.filter(col(c).isNull() | isnan(col(c))).count()\n",
    "    else:\n",
    "        count_missing = df.filter(col(c).isNull()).count()\n",
    "    if count_missing > 0:\n",
    "        missing_counts[c] = count_missing\n",
    "\n",
    "# Affichage propre\n",
    "import pandas as pd\n",
    "pd.DataFrame.from_dict(missing_counts, orient=\"index\", columns=[\"Valeurs manquantes\"]).sort_values(\"Valeurs manquantes\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c624a0d-aeb4-459c-a977-d91834e9f08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------+\n",
      "|TOP_PACK                                 |\n",
      "+-----------------------------------------+\n",
      "|On net 200F=Unlimited _call24H           |\n",
      "|On-net 1000F=10MilF;10d                  |\n",
      "|Data:1000F=5GB,7d                        |\n",
      "|Mixt 250F=Unlimited_call24H              |\n",
      "|MIXT:500F= 2500F on net _2500F off net;2d|\n",
      "|All-net 500F=2000F;5d                    |\n",
      "|On-net 500F_FNF;3d                       |\n",
      "|On net 200F=Unlimited _call24H           |\n",
      "|All-net 500F=2000F;5d                    |\n",
      "|Data: 100 F=40MB,24H                     |\n",
      "+-----------------------------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "+-----------+\n",
      "|REGION     |\n",
      "+-----------+\n",
      "|FATICK     |\n",
      "|DAKAR      |\n",
      "|DAKAR      |\n",
      "|LOUGA      |\n",
      "|LOUGA      |\n",
      "|DAKAR      |\n",
      "|DAKAR      |\n",
      "|TAMBACOUNDA|\n",
      "|DAKAR      |\n",
      "|KAOLACK    |\n",
      "+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"TOP_PACK\").filter(col(\"TOP_PACK\").isNotNull()).show(10, truncate=False)\n",
    "df.select(\"REGION\").filter(col(\"REGION\").isNotNull()).show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "abce464e-bee4-4ef0-9fde-dfc9f755490e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mode non trouvé pour TOP_PACK → remplacé par 'inconnu'\n",
      "Mode non trouvé pour REGION → remplacé par 'NON_PRECISÉ'\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, lower, upper, when\n",
    "\n",
    "# Nettoyage de base\n",
    "df = df.withColumn(\"TOP_PACK\", trim(lower(col(\"TOP_PACK\"))))\n",
    "df = df.withColumn(\"REGION\", trim(upper(col(\"REGION\"))))  # on standardise en majuscule\n",
    "\n",
    "# Remplacer les chaînes vides ou \"nan\"\n",
    "df = df.withColumn(\"TOP_PACK\", when((col(\"TOP_PACK\") == \"\") | (col(\"TOP_PACK\") == \"nan\"), None).otherwise(col(\"TOP_PACK\")))\n",
    "df = df.withColumn(\"REGION\", when((col(\"REGION\") == \"\") | (col(\"REGION\") == \"nan\"), None).otherwise(col(\"REGION\")))\n",
    "\n",
    "# Tentative de remplissage avec le mode\n",
    "def fill_with_mode_or_default(df, col_name, default_val):\n",
    "    mode_row = df.groupBy(col_name).count().orderBy(col(\"count\").desc()).first()\n",
    "    if mode_row and mode_row[0] is not None:\n",
    "        df = df.fillna({col_name: mode_row[0]})\n",
    "        print(f\"Rempli {col_name} avec son mode : {mode_row[0]}\")\n",
    "    else:\n",
    "        df = df.fillna({col_name: default_val})\n",
    "        print(f\"Mode non trouvé pour {col_name} → remplacé par '{default_val}'\")\n",
    "    return df\n",
    "\n",
    "# Application\n",
    "df = fill_with_mode_or_default(df, \"TOP_PACK\", \"inconnu\")\n",
    "df = fill_with_mode_or_default(df, \"REGION\", \"NON_PRECISÉ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5cbcb842-6369-4a31-86e1-8a3df8dda13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# Création des indexeurs pour chaque variable catégorielle\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=c, outputCol=c + \"_idx\", handleInvalid=\"keep\")\n",
    "    for c in cat_vars\n",
    "]\n",
    "\n",
    "# Application de chaque indexeur en séquence\n",
    "for indexer in indexers:\n",
    "    df = indexer.fit(df).transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2d3a0157-b4aa-45e4-91bc-697f3d4bd92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# On fusionne toutes les colonnes numériques + encodées\n",
    "cols_for_features = num_vars + [c + \"_idx\" for c in cat_vars]\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=cols_for_features,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "df_vector = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "255fba16-2ab3-400c-b105-f1b4cd3089d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"features_scaled\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")\n",
    "\n",
    "scaler_model = scaler.fit(df_vector)\n",
    "df_scaled = scaler_model.transform(df_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "250c56b1-0907-4455-a747-a0618125c59f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance expliquée par chaque composante principale :\n",
      "[0.4247323  0.12722984 0.08357809 0.07120151 0.0614992 ]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "\n",
    "# Réduction de la dimension avec PCA\n",
    "pca = PCA(k=5, inputCol=\"features_scaled\", outputCol=\"pca_features\")\n",
    "pca_model = pca.fit(df_scaled)\n",
    "df_pca = pca_model.transform(df_scaled)\n",
    "\n",
    "# Affichage de la variance expliquée\n",
    "print(\"Variance expliquée par chaque composante principale :\")\n",
    "print(pca_model.explainedVariance.toArray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "24920ec9-f4fc-42d6-bf51-fedb0667eba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|cluster|  count|\n",
      "+-------+-------+\n",
      "|      0|1062120|\n",
      "|      1|  55181|\n",
      "|      2| 318295|\n",
      "|      3|  23618|\n",
      "|      4| 694834|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Apllication du KMeans pour le clustering\n",
    "kmeans = KMeans(featuresCol=\"pca_features\", predictionCol=\"cluster\", k=5, seed=42)\n",
    "\n",
    "kmeans_model = kmeans.fit(df_pca)\n",
    "df_clustered = kmeans_model.transform(df_pca)\n",
    "\n",
    "# Affichage d’un aperçu des clusters attribués\n",
    "df_clustered.select(\"cluster\").groupBy(\"cluster\").count().orderBy(\"cluster\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "60811d70-cf25-4c5a-a5b6-553e9eb589e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-----------+---------+-----------+---------+-----------+----------+---------+----------+------------+-----------+-------+\n",
      "|cluster|REGION_idx|TENURE_idx|MONTANT    |FREQ_RECH|REVENUE    |FREQUENCE|DATA_VOLUME|ON_NET    |ORANGE   |REGULARITY|TOP_PACK_idx|FREQ_TOPACK|CHURN  |\n",
      "+-------+----------+----------+-----------+---------+-----------+---------+-----------+----------+---------+----------+------------+-----------+-------+\n",
      "|0      |3.17274   |0.10171   |2816.40804 |6.21965  |2723.33106 |7.99654  |2263.88937 |129.59871 |49.47416 |31.83874  |5.04479     |5.196      |0.00416|\n",
      "|1      |2.32165   |0.09878   |28512.2377 |49.51551 |29158.44231|53.33987 |13681.71764|736.00987 |607.72539|60.05596  |7.78413     |43.4925    |0.00783|\n",
      "|2      |2.68634   |0.1018    |10771.66017|22.35367 |10987.15423|27.16735 |6438.51295 |419.19241 |163.38082|55.1273   |7.13304     |16.10517   |0.00662|\n",
      "|3      |3.38839   |0.10793   |15217.72205|40.53507 |15576.65733|46.14891 |2624.08721 |5217.75532|213.90184|59.57249  |6.89178     |31.18146   |0.00694|\n",
      "|4      |0.23466   |0.15475   |5128.95271 |10.68388 |5042.9424  |12.8603  |2850.5802  |234.92345 |89.80398 |6.21826   |0.45147     |8.91142    |0.57117|\n",
      "+-------+----------+----------+-----------+---------+-----------+---------+-----------+----------+---------+----------+------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import round, avg, col\n",
    "\n",
    "# Moyenne des indicateurs par cluster, arrondis à 5 décimales\n",
    "df_clustered.groupBy(\"cluster\").agg(\n",
    "    round(avg(\"REGION_idx\"), 5).alias(\"REGION_idx\"),\n",
    "    round(avg(\"TENURE_idx\"), 5).alias(\"TENURE_idx\"),\n",
    "    round(avg(\"MONTANT\"), 5).alias(\"MONTANT\"),\n",
    "    round(avg(\"FREQUENCE_RECH\"), 5).alias(\"FREQ_RECH\"),\n",
    "    round(avg(\"REVENUE\"), 5).alias(\"REVENUE\"),\n",
    "    round(avg(\"FREQUENCE\"), 5).alias(\"FREQUENCE\"),\n",
    "    round(avg(\"DATA_VOLUME\"), 5).alias(\"DATA_VOLUME\"),\n",
    "    round(avg(\"ON_NET\"), 5).alias(\"ON_NET\"),\n",
    "    round(avg(\"ORANGE\"), 5).alias(\"ORANGE\"),\n",
    "    round(avg(\"REGULARITY\"), 5).alias(\"REGULARITY\"),\n",
    "    round(avg(\"TOP_PACK_idx\"), 5).alias(\"TOP_PACK_idx\"),\n",
    "    round(avg(\"FREQ_TOP_PACK\"), 5).alias(\"FREQ_TOPACK\"),\n",
    "    round(avg(\"CHURN\"), 5).alias(\"CHURN\")\n",
    ").orderBy(\"cluster\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "50537bf9-7c53-4393-b4de-595780ff8829",
   "metadata": {},
   "source": [
    "# INTERPRETATION DES SEGMENTS DES CLIENTS\n",
    "\n",
    "Cluster 0 – Clients silencieux à forte consommation de data\n",
    "- Revenu très faible, très peu de recharges\n",
    "- Data très élevée (~22 Go/mois), usage voix modéré\n",
    "- Très faible churn\n",
    "=> Recommandation : Packs Data-only bon marché, réengagement via bonus silencieux\n",
    "\n",
    "Cluster 1 – Clients premium fidèles et fortement engagés\n",
    "- Revenu très élevé, recharges très fréquentes\n",
    "- Usage voix et data très élevé\n",
    "- Très fidèle, très faible churn\n",
    "=> Recommandation : Programme VIP, cross-selling, bonus de fidélité, services premium\n",
    "\n",
    "Cluster 2 – Clients stables et multi-usages\n",
    "- Revenu et recharge moyens, usage équilibré\n",
    "- Fidélité correcte, faible churn\n",
    "=> Recommandation : Packs combinés, monitoring, montée en gamme modérée\n",
    "\n",
    "Cluster 3 – Utilisateurs vocaux réguliers\n",
    "- Faible usage data, appels voix intenses\n",
    "- Haute fidélité aux packs, régularité élevée\n",
    "=> Recommandation : Packs voix ciblés, introduction au digital, bonus Mo sur recharge\n",
    "\n",
    "Cluster 4 – Clients désengagés et à fort risque de churn\n",
    "- Très faible revenu, recharge rare, régularité quasi nulle\n",
    "- Churn extrêmement élevé (> 57%)\n",
    "=> Recommandation : Campagne de réactivation, bonus de retour, ou abandon ciblé\n",
    "\n",
    "Conclusion :\n",
    "• 2 clusters très fidèles à valoriser (cluster 1 et 3)\n",
    "• 2 segments à convertir (cluster 0 et 2)\n",
    "• 1 segment critique (cluster 4) à traiter d'urgence ou à laisser sortir"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
